# 006 决策树

## 1 问题描述
基本要求
1. 基于 Watermelon-train1 数据集(只有离散属性)构造 ID3 决策树;
2. 基于构造的 ID3 决策树,对数据集 Watermelon-test1 进行预测,输 出分类精度;
中级要求
1. 基于 Watermelon-train2 数据集构造 C4.5或者 CART 决策树,要求 可以处理连续型属性;
2. 基于构造的决策树,对数据集 Watermelon-test2 进行预测,输出分类精度;
高级要求
使用任意的剪枝算法对构造的决策树进行剪枝,观察测试集合的分类精度是否有提升,给出分析过程;
### 1.1 数据描述

有一个label， 是否是好瓜， 取值是[是, 否]。

有多个attri， 数据1的attris全是离散的， 每个attr有三个取值

数据2 有一个attr 密度 是连续的。 

值得注意的是训练集和测试集都不大， 训练集不大意味着决策树比较小， 测试集不大意味着预测正确率比较容易受特殊值影响而达不到一个好看的值。



## 2 解决方法
### 2.1 解决思路

1. 理解决策树建立过程， 写一个建立决策树的函数， 需要注意的有两点
   1. 计算信息增益的接口留出， 方便单独实现，也方便使用不同实现的函数
   2. 对于连续值有不同的处理方法
2. 实现信息增益的计算函数， 主要是要理解公式
3. 实现剪枝函数

### 2.2 基本理论

#### 2.2.1 决策树与决策树构建

决策树(decision tree)是一类常见的机器学习算法。 核心的思路是把从attris推断label看作一次决策， 然后接下来把这个决策一步步分成子决策。 以判断瓜甜不甜为例子， 我们想通过我们对attris[色泽,根蒂,敲声,纹理,密度]的了解决策这个瓜是否好瓜， 那么我们先看颜色是不是好， 再看敲声是不是好， 最终能在决策树的叶节点下一个判断--是好瓜或者不是好瓜。

形式化的定义， 一棵决策树有一些叶节点和非叶节点， 叶节点上要标明label，而非叶节点则对应一个决策attr， 依据这个attr的取值，每一个取值产生一颗对应的子树。

~~~pseudocode
输入
训练集D={(x1,y1),(x2,y2),…(xm,ym)};
属性集A={a1,a2,…ad}.
过程：函数TreeGenerate(D,A, node)
    if D中样本全属于同一类别C：
        将node标记为C类叶结点；
        return 
    end if
    if A=空集 or D中样本在A上取值相同：
        将node标记为D中样本数(当前结点)最多的类(成为叶结点)；
        return 
    end if
    从A中选择最优划分属性a*：
    for a* 的每个值 a‘* do
        为node生成一个分支；令Dv表示D中在a*上取值为a’*的样本子集
        if Dv为空：
            将分支结点标记为D中样本数(父结点)最多的类(成为叶结点);
            return 
        else
        	TreeGenerate（Dv，A{a*},child_of_node）
        end if
    end for
输出
以node为根结点的一棵决策树

~~~



#### 2.2.2 划分选择

#### 2.2.4 连续值处理

#### 2.2.5 预剪枝

#### 2.2.6 剪枝

### 2.3 算法分析
## 3 实验分析
